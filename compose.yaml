name: cedrik
services:
  backendbase:
    build:
      dockerfile: Backend.Dockerfile
    image: cedrik-base
    pull_policy: never
    profiles: ["base"]
  backend:
    build:
      dockerfile: BackendRunner.Dockerfile
    image: maasuncion/cedrik-backend:latest
    profiles: ["backend"]
  frontend:
    build:
      context: ./frontend/
      dockerfile: Dockerfile
    # profiles: ["frontend"]
    command: ["nginx", "-g", "daemon off;"]
    volumes:
      - ./frontend/.env:/app/.env:ro
    ports:
      - 127.0.0.1:8080:80
    networks:
        - internal
    restart: unless-stopped

  main:
    image: maasuncion/cedrik-backend:latest
    # pull_policy: never
    volumes:
      - ./backend/:/app/backend/
      - ./log/:/app/log/
      - ./.env:/app/.env:ro
      - ./tokenizer_config.json:/app/tokenizer_config.json:ro
      - ./pipe_config.json:/app/pipe_config.json:ro
    command: [ "/bin/sh", "-c", "flask run $${DEBUG} $${NO_RELOAD}" ]
    # Only one process for main, mongoengine document writes is not thread safe
    # command: [ "/bin/sh", "-c", "uwsgi --http $${FLASK_RUN_HOST}:$${FLASK_RUN_PORT} --master --processes 1 --threads 1 -w $${FLASK_APP}:app" ]
    # command: [ "/bin/bash" ]
    # stdin_open: true
    # tty: true
    networks:
        - internal
    ports:
      - 127.0.0.1:5000:5000
    environment:
      FLASK_RUN_HOST: 0.0.0.0
      FLASK_RUN_PORT: 5000
      FLASK_APP: backend.Apps.Main
      DEBUG: --debug
      NO_RELOAD: --no-reload
    develop:
      watch:
        - path: ./backend/
          action: restart
    restart: unless-stopped
  encoder:
    image: maasuncion/cedrik-backend:latest
    # pull_policy: never
    volumes:
      - ./backend/:/app/backend/
      - ./log/:/app/log/
      - ./.env:/app/.env:ro
      - ./tokenizer_config.json:/app/tokenizer_config.json:ro
      - ./pipe_config.json:/app/pipe_config.json:ro
      - ./hf_cache:/app/hf_cache
    # command: [ "/bin/sh", "-c", "flask run $${DEBUG} $${NO_RELOAD}" ]

    # Reduce processes for encoder, model, filter-model (to reduce load of server)
    command: [ "/bin/sh", "-c", "uwsgi --http $${FLASK_RUN_HOST}:$${FLASK_RUN_PORT} --master --processes 2 --threads 4 -w $${FLASK_APP}:app" ]
    networks:
        - internal
    # ports:
    #   - 5001:5001
    environment:
      FLASK_RUN_HOST: 0.0.0.0
      FLASK_RUN_PORT: 5001
      FLASK_APP: backend.Apps.Encoder
      DEBUG: --debug
      NO_RELOAD: --no-reload
      HF_HOME: /app/hf_cache
    restart: unless-stopped
  model:
    image: maasuncion/cedrik-backend:latest
    # pull_policy: never
    volumes:
      - ./backend/:/app/backend/
      - ./log/:/app/log/
      - ./.env:/app/.env:ro
      - ./tokenizer_config.json:/app/tokenizer_config.json:ro
      - ./pipe_config.json:/app/pipe_config.json:ro
      - ./hf_cache:/app/hf_cache
    # command: [ "/bin/sh", "-c", "flask run $${DEBUG} $${NO_RELOAD}" ]
    command: [ "/bin/sh", "-c", "uwsgi --http $${FLASK_RUN_HOST}:$${FLASK_RUN_PORT} --master --processes 2 --threads 4 -w $${FLASK_APP}:app" ]
    networks:
      - internal
    # ports:
    #   - 5002:5002
    environment:
      FLASK_RUN_HOST: 0.0.0.0
      FLASK_RUN_PORT: 5002
      FLASK_APP: backend.Apps.Model
      DEBUG: --debug
      NO_RELOAD: --no-reload
      HF_HOME: /app/hf_cache
    restart: unless-stopped
  filter-model:
    image: maasuncion/cedrik-backend:latest
    # pull_policy: never
    volumes:
      - ./backend/:/app/backend/
      - ./log/:/app/log/
      - ./.env:/app/.env:ro
      - ./tokenizer_config.json:/app/tokenizer_config.json:ro
      - ./pipe_config.json:/app/pipe_config.json:ro
      - ./hf_cache:/app/hf_cache
    # command: [ "/bin/sh", "-c", "flask run $${DEBUG} $${NO_RELOAD}" ]
    command: [ "/bin/sh", "-c", "uwsgi --http $${FLASK_RUN_HOST}:$${FLASK_RUN_PORT} --master --processes 2 --threads 4 -w $${FLASK_APP}:app" ]
    networks:
      - internal
    # ports:
    #   - 5003:5003
    environment:
      FILTER_MODE: 1
      FLASK_RUN_HOST: 0.0.0.0
      FLASK_RUN_PORT: 5003
      FLASK_APP: backend.Apps.Model
      DEBUG: --debug
      NO_RELOAD: --no-reload
      HF_HOME: /app/hf_cache
    restart: unless-stopped

  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    profiles: ["llama"]
    # pull_policy: never
    entrypoint: "/bin/sh"
    command: [ "-c", "/app/llama-server -m $${MODEL} --host $${HOST} --port $${PORT} --threads 8 --n-gpu-layers 0 --ctx-size 1024" ]
    # command: [ "/bin/sh", "-c", "uwsgi --http $${FLASK_RUN_HOST}:$${FLASK_RUN_PORT} --master --processes 4 --threads 4 -w $${FLASK_APP}:app" ]
    # stdin_open: true
    # tty: true
    volumes:
      - ./models/:/app/models/
    networks:
      - internal
    # ports:
    #   - 8080:8080
    environment:
      MODEL: /app/models/Qwen2.5-VL-7B-Instruct-UD-IQ1_M.gguf
      HOST: 0.0.0.0
      PORT: 8080

networks:
  internal:
